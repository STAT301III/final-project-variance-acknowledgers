---
title: "Final Writeup"
author: "Will Cichowski, Olivia Lee, Matthew Lorenz, Louis Yang"
date: '2022-06-06'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, message = FALSE, echo = FALSE}
library(tidyverse)
library(tidymodels)
library(skimr)
library(janitor)
library(hutils)

tidymodels_prefer()
```

# Initial EDA

## Introduction

This data was found on Kaggle (https://www.kaggle.com/datasets/deepcontractor/unicorn-companies-dataset). 

This dataset has details on various unicorn companies, a private company or startup that has a valuation over 1 billion. Former unicorn companies include Facebook, Airbnb, and Google. Being a "unicorn company" has value because it is so rare: of software startups in the 2000s, only 0.07% reached the 1 billion valuation mark. It signals a company that has grown quickly and one that is headed for success. This dataset has information of the 1000 unicorn companies in the world, as of March 2022. It has variables like current valuation, data created, country, city, industry, financial stage and more. 

We think there are three main research question that can be explored. The first: given various characteristics of these companies, what is their valuation? This will entail a regression based approach. The second: what stage of financing is the startup in given various characteristics about the company. This would be a classification model. The third: what industry is the startup in?

Number of observations: 762
Number of features: 91

Missingness check:

```{r}
#read in raw data
uni <- read.csv('data/unprocessed/Unicorn_Companies.csv')

#Do a quick skim of the data
skim_without_charts(uni)
```


Here, we can see that there is no missingness in the data at all, but upon inspecting the data that is because any missing values contain a character "None" in lieu of an NA.

## Data Cleaning/Preprocessing
```{r, message = FALSE, warning = FALSE}
#Let's first turn the numerical columns that are currently characters to numerical:
uni_clean <- uni %>%
  mutate(valuation = as.numeric(extract_numeric(Valuation...B.)) * 1000000000,
         date_joined= extract_numeric(str_sub(Date.Joined, start= -4)),
         investor_count = extract_numeric(Investors.Count),
         deal_terms = extract_numeric(Deal.Terms),
         portfolio_exits = extract_numeric(Portfolio.Exits),
         total_raised = extract_numeric(Total.Raised) * 1000000000,
         Select.Inverstors = str_split(Select.Inverstors, ", ")
  ) %>%
  unnest(cols = c(Select.Inverstors)) %>% 
  select(-c(Date.Joined, Investors.Count, Valuation...B., Deal.Terms, Portfolio.Exits, Total.Raised)) %>%
  clean_names()

# put in dummy column
uni_clean <- data.frame(append(uni_clean, c(x1="1"), after=5))

# pivot wide
uni_clean <- uni_clean %>% 
  group_by(company, country, city, industry, founded_year, financial_stage, valuation, date_joined, investor_count, deal_terms, portfolio_exits, total_raised, select_inverstors) %>%
  mutate(rn = row_number()) %>% 
  pivot_wider(
    names_from = select_inverstors,
    values_from = x1
  ) 

# put in 0s
uni_clean <- uni_clean %>% 
  ungroup() %>% 
  mutate_all(~replace(., is.na(.), 0)) %>% 
  select(-rn) %>% 
  clean_names()

uni_clean_visual <- read_csv(file = "data/processed/uni_clean.csv")

# remove investor columns w less than 10 dummys
uni_clean <- bind_cols(uni_clean[1], uni_clean[-1] %>% select_if(funs(sum(. > 0) >= 10)))

skim_without_charts(uni_clean)
```

This is the main data preprocessing step, one that is crucial for this dataset. Given how unorganized the investors column was, the investor column was separated out into multiple rows, based on how many investors there actually were, and then recombined into one while being dummy coded. We also did not dummy code investors that did not invest in 10 or more companies as this would just add noise. All of the data that was inputted as characters was made into numbers as well. The skim that was performed on the dataset shows us that there are no missing values. Any NAs present in the data were converted to 0s.

## Essential Findings

### Univariate Analysis

#### Response Variables

```{r}
ggplot (uni_clean) +
  geom_histogram(aes(x = valuation), bins = 50)
```

Looking at the distribution of the valuation, we can see that the data is right skewed and centered around 1 billion. That is where the majority of observation can be seen and it makes sense that there is no left skew or even any values to the left of 1 billion because we are looking at unicorn companies. It means this data set will only have values above 1 billion. While it might seem obvious that a data set of unicorn companies by definition should have valuations over $1B, this is important to confirm as we're familiarizing ourselves with the data.


```{r}
# bar plot for industries
uni_clean_visual %>% 
  group_by(industry) %>%
  count() %>%
  filter(n > 20) %>%
  ggplot(aes(x = reorder(industry, -n), y = n)) + 
  geom_bar(stat = "identity", color = "red", fill = "orange") +
  coord_flip() +
  theme_minimal() +
  labs(
    x = "Industry", 
    y = "Count"
  ) 
```

Looking at another potential response variable in industry, we see that Fintech has the most observations. The second biggest we see is Interset software and services with other big industries including E-commerce, data management/analytics, cybersecurity, health, and AI. There is enough diversity in industries to conduct a thorough analysis. The majority of these are one off industries. Given this, we only showed industries on this graph with more than 20 observations. The rest are reflected in the other category.

```{r, warning = FALSE}
ggplot (uni_clean) +
  geom_histogram(aes(x = financial_stage), stat = "count") +
  coord_flip()
```

We now look at our final potential response variable: the financial stage. It appears that this one will be much harder to predict as the overwhelming majority of observations don't have a financial stage listed. One reason we can think of for this is that it can be hard to discern, especially for private companies, what stage that they may be in. There may be multiple fundraising rounds and each company matures differently. 

#### Predictors

```{r}
ggplot (uni_clean) +
  geom_histogram(aes(x = investor_count), bins = 50)
```

Looking at the distribution of investor count of these unicorn companies, there is a rightward skew and the data is centered around 12 investors. There are outliers that have investors of over 75. We hypothesize the number of investors to be important as it gives a look into how much attention the company has drawn and how mature it may be when it comes to the valuation. 

```{r, message = FALSE}
uni_clean_visual %>% 
  mutate(country = fct_lump(country %>% as.factor, n=5)) %>% 
  group_by(country) %>%
  count() %>%
  ggplot(aes(x = "", y = n, fill = country)) + 
  geom_bar(stat = "identity") +
  coord_flip() +
  theme_minimal() +
  coord_polar(theta = "y") +
  labs(
    x = NULL,
    y = NULL
  )
```

We look at the countries of each of these companies to understand if there could be a relationship between countries and a response variable like valuation. We can see that the US has an overwhelming majority of these unicorn companies with China a distant second. It gives an insight as to which countries it may be more likely that a unicorn company appears. This pie chart shows how these companies are distributed and it makes sense with our own intuition, given the startups that we know to be successful tend to be from China or the US. Companies like Bytedance or Fanatics all come to mind when thinking about big time unicorn companies.


```{r, warning = FALSE}
ggplot (uni_clean) +
  geom_histogram(aes(x = founded_year), stat = "count") +
  coord_flip()
```


First off, there is a leftward skew in this data but it is distributed normally overall. It is centered at 2015 which gives an idea into how long it takes a usual unicorn company to mature and reach unicorn status. 

### Bivariate Analysis

#### Response Variable and Predictor Variable Relationships

```{r, message = FALSE}
ggplot (uni_clean, aes(x = investor_count, y = valuation)) +
  geom_point() +
  geom_smooth(method ='lm')
```

Looking at the graph between investor count and valuation, there appears to be a slightly positive correlation between the two variables. There definitely are outliers but most of the data is clustered in the third quadrant, where investors are below 50 and valuation just above 1 billion. 

```{r, message = FALSE}
ggplot (uni_clean, aes(x = total_raised, y = valuation)) +
  geom_point() +
  geom_smooth(method ='lm')
```

Looking at this graph, we see that there really is no relationship between total raised and valuation. It appears to be a rather unimpactful predictor as the data is almost normally distributed.

```{r}
ggplot(uni_clean, aes(x = reorder(founded_year, -valuation), y = valuation)) +
  geom_bar(aes(y = valuation), stat = "identity") +
  coord_flip()
```

Looking at this graph shows us that many current unicorn companies with higher valuations were founded in the 2010s.

```{r}
ggplot(uni_clean, aes(x = reorder(industry, -valuation), y = valuation)) +
  geom_bar(aes(y = valuation), stat = "identity") +
  coord_flip()
```

We see some interesting insights when looking at this graph, Fintech is the industry that has the highest valued companies. In second, it is internet software and services. This is an interesting insight that gives a different angle than the univariate analysis on industries did. The univariate analysis told us that internet software and services had the most amount of companies. We see that most number of companies does not equate to highest valuations.

```{r}
ggplot(uni_clean, aes(x = reorder(country, -valuation), y = valuation)) +
  geom_bar(aes(y = valuation), stat = "identity") +
  coord_flip()
```

This analysis is in line with the country univariate analysis. The US has the highest valued companies, then China, then India. 

#### Relationships among predictor variables

```{r, message = FALSE}
ggplot (uni_clean, aes(x = investor_count, y = total_raised)) +
  geom_point() +
  geom_smooth(method ='lm')
```

There is really no relationship between the number of investors in a company and the total amount raised. We see a very slight positive correlation and this can probably be attributed to the fact that a single investment entity can make a massive investment compared to others.

```{r, message = FALSE}
ggplot (uni_clean, aes(x = deal_terms, y = total_raised)) +
  geom_point() +
  geom_smooth(method ='lm')
```

We see that there is a lack of a relationship between the number of deal terms and the total raised.

```{r}
ggplot(uni_clean, aes(x = industry, fill = country)) + 
  geom_bar(position = "stack") 
```

When using this stacked bar graph to analyze categorical variables of industry and country, we see that in both fintech and internet services the US takes up a big fraction of the companies. Going forward when doing analysis, we will have to eliminate some of the smaller companies and possibly create a threshold for the countries we analyze as it is hard to see relationships with so many countries.

## Conclusions From Initial EDA

From this initial exploratory data analysis, we have achieved a deeper understand of the data at hand. We've learned that there is a higher proportion of internet software and services followed by financial technology companies within the collective of businesses valued over $1B, but there is significant diversity overall in terms of industry.

Moving forward, some variables we are paying close attention to are the investor count, country of origin, and year founded due to the distributions seen above. We saw a slight correlation between a company's valuation and the number of investors, but it is unclear whether that correlation is significant due to the clustering behavior of the data. Additionally, our bivariate predictor analysis yielded very low correlation, which is promising for our eventual model because we seek exogenous variables for prediction.

# Developing Models

## Natural Language Processing

Natural Language Processing (NLP) is the method of transforming text data into usable representations for modeling. Text data proves challenging to work with because of the multitude of elements involved in linguistics, with information ranging from phonetic, the individual breakdown of sounds, to pragmatic, how language is used in context.

source: https://smltar.com

Because our data includes text information, specifically 13 character-type variables, we wanted to explore using NLP to see if it increases the efficacy of our model's predictions. However, NLP might not be necessary if we decide that converting the character variables to factors is as effective, especially since factor variables are more familiar and are therefore easier for us to model.

With NLP, it is important for us to consider our method of tokenization, which is the process of separating text-based variables into smaller units, like separating a sentence into a vector of words. NLP and tokenization are most relevant in the select investors variable.

Since our target variable is a company's valuation, it is crucial that we treat the investors variable intentionally. From general knowledge, we know there is a correlation between which investors decide a company is worth devoting resources to and the eventual success of a company. That is, experienced investors likely know the criteria for a likely successful company and decide to invest based on their in-house standards.

Upon our initial look at the data, we noticed that certain funds of the same name were categorized as different investor groups because of small syntactical differences. To address this, some form of NLP is necessary.


## Recipe Creation
```{r, eval = FALSE}
# recipe with no natural language processing
recipe_no_nlp <- recipe(valuation ~ ., data = exp_1_train) %>% 
  step_rm(company, portfolio_exits) %>% 
  step_novel() %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_normalize(all_numeric_predictors())

# recipe with NLP
recipe_no_nlp <- recipe(valuation ~ ., data = exp_1_train) %>% 
  step_rm(company, portfolio_exits) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_normalize(all_numeric_predictors())

```

We created a recipe for an NLP and non NLP model. In order to prepare for the feature engineering process, as mentioned earlier, all of the investors were dummy coded. This is important to do as there are going to be investment firms that consistently make better investments than other companies. This was done for the investment companies that invested in at least 10 different unicorn companies in the dataset. On top of this, in both the NLP and non NLP recipes, all of the other categorical variables were dummy coded and normalized. We determined from our EDA that the number of portfolio exits was not significant either so that feature was removed.

## Data Splitting
```{r}
set.seed(3013)
exp_1_split <- initial_split(uni_clean, strata = valuation)
exp_1_train <- training(exp_1_split) 
exp_1_test <- testing(exp_1_split)

exp_1_folds <- vfold_cv(exp_1_train, strata = valuation, v = 5)
```



WRITE HERE


## Model Fitting


WRITE HERE









