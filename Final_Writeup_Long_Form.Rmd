---
title: "Final Writeup"
author: "Will Cichowski, Olivia Lee, Matthew Lorenz, Louis Yang"
date: '2022-06-06'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, message = FALSE, echo = FALSE}
library(tidyverse)
library(tidymodels)
library(skimr)
library(janitor)
library(hutils)

tidymodels_prefer()
```

# Initial EDA

## Introduction

This data was found on Kaggle (https://www.kaggle.com/datasets/deepcontractor/unicorn-companies-dataset). 

This dataset has details on various unicorn companies, a private company or startup that has a valuation over 1 billion. Former unicorn companies include Facebook, Airbnb, and Google. Being a "unicorn company" has value because it is so rare: of software startups in the 2000s, only 0.07% reached the 1 billion valuation mark. It signals a company that has grown quickly and one that is headed for success. This dataset has information of the 1000 unicorn companies in the world, as of March 2022. It has variables like current valuation, data created, country, city, industry, financial stage and more. 

We think there are three main research question that can be explored. The first: given various characteristics of these companies, what is their valuation? This will entail a regression based approach. The second: what stage of financing is the startup in given various characteristics about the company. This would be a classification model. The third: what industry is the startup in?

Number of observations: 762
Number of features: 91

Missingness check:

```{r}
#read in raw data
uni <- read.csv('data/unprocessed/Unicorn_Companies.csv')

#Do a quick skim of the data
skim_without_charts(uni)
```


Here, we can see that there is no missingness in the data at all, but upon inspecting the data that is because any missing values contain a character "None" in lieu of an NA.

## Data Cleaning/Preprocessing
```{r, message = FALSE, warning = FALSE}
#Let's first turn the numerical columns that are currently characters to numerical:
uni_clean <- uni %>%
  mutate(valuation = as.numeric(extract_numeric(Valuation...B.)) * 1000000000,
         date_joined= extract_numeric(str_sub(Date.Joined, start= -4)),
         investor_count = extract_numeric(Investors.Count),
         deal_terms = extract_numeric(Deal.Terms),
         portfolio_exits = extract_numeric(Portfolio.Exits),
         total_raised = extract_numeric(Total.Raised) * 1000000000,
         Select.Inverstors = str_split(Select.Inverstors, ", ")
  ) %>%
  unnest(cols = c(Select.Inverstors)) %>% 
  select(-c(Date.Joined, Investors.Count, Valuation...B., Deal.Terms, Portfolio.Exits, Total.Raised)) %>%
  clean_names()

# put in dummy column
uni_clean <- data.frame(append(uni_clean, c(x1="1"), after=5))

# pivot wide
uni_clean <- uni_clean %>% 
  group_by(company, country, city, industry, founded_year, financial_stage, valuation, date_joined, investor_count, deal_terms, portfolio_exits, total_raised, select_inverstors) %>%
  mutate(rn = row_number()) %>% 
  pivot_wider(
    names_from = select_inverstors,
    values_from = x1
  ) 

# put in 0s
uni_clean <- uni_clean %>% 
  ungroup() %>% 
  mutate_all(~replace(., is.na(.), 0)) %>% 
  select(-rn) %>% 
  clean_names()

uni_clean_visual <- read_csv(file = "data/processed/uni_clean.csv")

# remove investor columns w less than 10 dummys
uni_clean <- bind_cols(uni_clean[1], uni_clean[-1] %>% select_if(funs(sum(. > 0) >= 10)))

skim_without_charts(uni_clean)
```

This is the main data preprocessing step, one that is crucial for this dataset. Given how unorganized the investors column was, the investor column was separated out into multiple rows, based on how many investors there actually were, and then recombined into one while being dummy coded. We also did not dummy code investors that did not invest in 10 or more companies as this would just add noise. All of the data that was inputted as characters was made into numbers as well. The skim that was performed on the dataset shows us that there are no missing values. Any NAs present in the data were converted to 0s.

## Essential Findings

### Univariate Analysis

#### Response Variables

```{r}
ggplot (uni_clean) +
  geom_histogram(aes(x = valuation), bins = 50)
```

Looking at the distribution of the valuation, we can see that the data is right skewed and centered around 1 billion. That is where the majority of observation can be seen and it makes sense that there is no left skew or even any values to the left of 1 billion because we are looking at unicorn companies. It means this data set will only have values above 1 billion. While it might seem obvious that a data set of unicorn companies by definition should have valuations over $1B, this is important to confirm as we're familiarizing ourselves with the data.


```{r}
# bar plot for industries
uni_clean_visual %>% 
  group_by(industry) %>%
  count() %>%
  filter(n > 20) %>%
  ggplot(aes(x = reorder(industry, -n), y = n)) + 
  geom_bar(stat = "identity", color = "red", fill = "orange") +
  coord_flip() +
  theme_minimal() +
  labs(
    x = "Industry", 
    y = "Count"
  ) 
```

Looking at another potential response variable in industry, we see that Fintech has the most observations. The second biggest we see is Interset software and services with other big industries including E-commerce, data management/analytics, cybersecurity, health, and AI. There is enough diversity in industries to conduct a thorough analysis. The majority of these are one off industries. Given this, we only showed industries on this graph with more than 20 observations. The rest are reflected in the other category.

```{r, warning = FALSE}
ggplot (uni_clean) +
  geom_histogram(aes(x = financial_stage), stat = "count") +
  coord_flip()
```

We now look at our final potential response variable: the financial stage. It appears that this one will be much harder to predict as the overwhelming majority of observations don't have a financial stage listed. One reason we can think of for this is that it can be hard to discern, especially for private companies, what stage that they may be in. There may be multiple fundraising rounds and each company matures differently. 

#### Predictors

```{r}
ggplot (uni_clean) +
  geom_histogram(aes(x = investor_count), bins = 50)
```

Looking at the distribution of investor count of these unicorn companies, there is a rightward skew and the data is centered around 12 investors. There are outliers that have investors of over 75. We hypothesize the number of investors to be important as it gives a look into how much attention the company has drawn and how mature it may be when it comes to the valuation. 

```{r, message = FALSE}
uni_clean_visual %>% 
  mutate(country = fct_lump(country %>% as.factor, n=5)) %>% 
  group_by(country) %>%
  count() %>%
  ggplot(aes(x = "", y = n, fill = country)) + 
  geom_bar(stat = "identity") +
  coord_flip() +
  theme_minimal() +
  coord_polar(theta = "y") +
  labs(
    x = NULL,
    y = NULL
  )
```

We look at the countries of each of these companies to understand if there could be a relationship between countries and a response variable like valuation. We can see that the US has an overwhelming majority of these unicorn companies with China a distant second. It gives an insight as to which countries it may be more likely that a unicorn company appears. This pie chart shows how these companies are distributed and it makes sense with our own intuition, given the startups that we know to be successful tend to be from China or the US. Companies like Bytedance or Fanatics all come to mind when thinking about big time unicorn companies.


```{r, warning = FALSE}
ggplot (uni_clean) +
  geom_histogram(aes(x = founded_year), stat = "count") +
  coord_flip()
```


First off, there is a leftward skew in this data but it is distributed normally overall. It is centered at 2015 which gives an idea into how long it takes a usual unicorn company to mature and reach unicorn status. 

### Bivariate Analysis

#### Response Variable and Predictor Variable Relationships

```{r, message = FALSE}
ggplot (uni_clean, aes(x = investor_count, y = valuation)) +
  geom_point() +
  geom_smooth(method ='lm')
```

Looking at the graph between investor count and valuation, there appears to be a slightly positive correlation between the two variables. There definitely are outliers but most of the data is clustered in the third quadrant, where investors are below 50 and valuation just above 1 billion. 

```{r, message = FALSE}
ggplot (uni_clean, aes(x = total_raised, y = valuation)) +
  geom_point() +
  geom_smooth(method ='lm')
```

Looking at this graph, we see that there really is no relationship between total raised and valuation. It appears to be a rather unimpactful predictor as the data is almost normally distributed.

```{r}
ggplot(uni_clean, aes(x = reorder(founded_year, -valuation), y = valuation)) +
  geom_bar(aes(y = valuation), stat = "identity") +
  coord_flip()
```

Looking at this graph shows us that many current unicorn companies with higher valuations were founded in the 2010s.

```{r}
ggplot(uni_clean, aes(x = reorder(industry, -valuation), y = valuation)) +
  geom_bar(aes(y = valuation), stat = "identity") +
  coord_flip()
```

We see some interesting insights when looking at this graph, Fintech is the industry that has the highest valued companies. In second, it is internet software and services. This is an interesting insight that gives a different angle than the univariate analysis on industries did. The univariate analysis told us that internet software and services had the most amount of companies. We see that most number of companies does not equate to highest valuations.

```{r}
ggplot(uni_clean, aes(x = reorder(country, -valuation), y = valuation)) +
  geom_bar(aes(y = valuation), stat = "identity") +
  coord_flip()
```

This analysis is in line with the country univariate analysis. The US has the highest valued companies, then China, then India. 

#### Relationships among predictor variables

```{r, message = FALSE}
ggplot (uni_clean, aes(x = investor_count, y = total_raised)) +
  geom_point() +
  geom_smooth(method ='lm')
```

There is really no relationship between the number of investors in a company and the total amount raised. We see a very slight positive correlation and this can probably be attributed to the fact that a single investment entity can make a massive investment compared to others.

```{r, message = FALSE}
ggplot (uni_clean, aes(x = deal_terms, y = total_raised)) +
  geom_point() +
  geom_smooth(method ='lm')
```

We see that there is a lack of a relationship between the number of deal terms and the total raised.

```{r}
ggplot(uni_clean, aes(x = industry, fill = country)) + 
  geom_bar(position = "stack") 
```

When using this stacked bar graph to analyze categorical variables of industry and country, we see that in both fintech and internet services the US takes up a big fraction of the companies. Going forward when doing analysis, we will have to eliminate some of the smaller companies and possibly create a threshold for the countries we analyze as it is hard to see relationships with so many countries.

## Conclusions From Initial EDA

From this initial exploratory data analysis, we have achieved a deeper understand of the data at hand. We've learned that there is a higher proportion of internet software and services followed by financial technology companies within the collective of businesses valued over $1B, but there is significant diversity overall in terms of industry.

Moving forward, some variables we are paying close attention to are the investor count, country of origin, and year founded due to the distributions seen above. We saw a slight correlation between a company's valuation and the number of investors, but it is unclear whether that correlation is significant due to the clustering behavior of the data. Additionally, our bivariate predictor analysis yielded very low correlation, which is promising for our eventual model because we seek exogenous variables for prediction.

# Developing Models

## Natural Language Processing

Natural Language Processing (NLP) is the method of transforming text data into usable representations for modeling. Text data proves challenging to work with because of the multitude of elements involved in linguistics, with information ranging from phonetic, the individual breakdown of sounds, to pragmatic, how language is used in context.

source: https://smltar.com

Because our data includes text information, specifically 13 character-type variables, we wanted to explore using NLP to see if it increases the efficacy of our model's predictions. However, NLP might not be necessary if we decide that converting the character variables to factors is as effective, especially since factor variables are more familiar and are therefore easier for us to model.

With NLP, it is important for us to consider our method of tokenization, which is the process of separating text-based variables into smaller units, like separating a sentence into a vector of words. NLP and tokenization are most relevant in the select investors variable.

Since our target variable is a company's valuation, it is crucial that we treat the investors variable intentionally. From general knowledge, we know there is a correlation between which investors decide a company is worth devoting resources to and the eventual success of a company. That is, experienced investors likely know the criteria for a likely successful company and decide to invest based on their in-house standards.

Upon our initial look at the data, we noticed that certain funds of the same name were categorized as different investor groups because of small syntactical differences. To address this, some form of NLP is necessary.


## Recipe Creation
```{r, eval = FALSE}
# recipe with no natural language processing
recipe_no_nlp <- recipe(valuation ~ ., data = exp_1_train) %>% 
  step_rm(company, portfolio_exits) %>% 
  step_novel() %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_normalize(all_numeric_predictors())

# recipe with NLP
recipe_no_nlp <- recipe(valuation ~ ., data = exp_1_train) %>% 
  step_rm(company, portfolio_exits) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_normalize(all_numeric_predictors())

```

We created a recipe for an NLP and non NLP model. In order to prepare for the feature engineering process, as mentioned earlier, all of the investors were dummy coded. This is important to do as there are going to be investment firms that consistently make better investments than other companies. This was done for the investment companies that invested in at least 10 different unicorn companies in the dataset. On top of this, in both the NLP and non NLP recipes, all of the other categorical variables were dummy coded and normalized. We determined from our EDA that the number of portfolio exits was not significant either so that feature was removed.

## Data Splitting
```{r}
set.seed(3013) 
exp_1_split <- initial_split(uni_clean, prop = .8, strata = valuation)
exp_1_train <- training(exp_1_split) 
exp_1_test <- testing(exp_1_split)

exp_1_folds <- vfold_cv(exp_1_train, strata = valuation, v = 2, r = 10)
```


To split our data, we chose an 80/20 split of training to testing data, something standard. We did not touch the testing set again until the evaluation of our models at the very end, and used only exp_1_train throughout the model fitting process. 

The v-folding process took a bit of finesse in order to make our model fitting work properly. As each of the investment firms ended up being dummy encoded, we ran into an issue with dimensionality. In addition, in our recipes, we dummy encoded all nominal variables, such as country. Once we baked our recipes, we noticed about 340 columns. The problem with this was that our training set only held 830 observations. As such, if we performed a v-fold with, say, five folds, each fold would have only 166 observations in it. With only 166 observations for 340 columns, many of them being rare, there was a good chance that we would end up with a lack of representation of certain dummy columns in our v folding process and many columns with zero variance. 

To handle this, we decided to do a v-fold of only 2 folds, but 10 repeats. 2 folds would help remedy the aforementioned issue of how many dimensions we had, while compensating for the lost advantages of having more folds by having a high number of repeats. With this, we could begin initializing and fitting our models. 


## Model Fitting

For our model fitting, we had a procedure for each of the experiments. Experiments 1 and 3, the regression problems, had the same process and model types. For each of these, we tried models of random forest, xgboost, and linear regression. For each of these models, we had two recipes (one with NLP and one without), and that resulted in six models total to fit. For the random forest, we used the ranger engine and tuned the mtry parameter. For xgboost we used the xgboost engine and tuned the mtry and min_n parameters. For the linear regression, we used the lm engine and tried to tune mixture and penalty parameters. 

Then, for our classification problem of experiment 2, we used random forest and xgboost as our model types, tuning the same parameters and using the same engines as stated above. This resulted in four models total, due to the two recipes. 

Using our v fold cross validation, we were able to perform a grid hypertuning on our tuning parameters and then, after choosing the best performing parameter value models, we fit that to the training data, and stored all of the models in the fit models folder. 


# Results


Once we had all our models fit to the training set, it was finally time to break out the testing set and evaluate. For the regression experiments, we used the root mean squared error and the mean absolute percent error metrics. The reasoning behind this was that RMSE could give us a dollar amount value on the general error our models had, which is useful having it in the practical base units. However, dealing with values in the billions can be somewhat hard to interpret, so we also included mean absolute percent error so that we could tell what factor we were off by. 

For the classification problem, we used the accuracy metric and the f measure Accuracy is very misleading for this experiment due to the extremely low proportion of values that are not None for financial stage, but we included it anyway because it still shows fluctuation between models and is somewhat of a base metric. For the F measure, this better demonstrates the model's ability to handle the non None values, as it includes things like false positives, which would punish always guessing None more than pure accuracy. 



```{r, echo = FALSE}
load('results/metric_tables.rda')
```


```{r}
exp_1_table
```
Here, we can see the table we built to evaluate our models in experiment 1. The best performing model is the random forest using the recipe that included NLP. It had the lowest MAPE and RMSE. It is interesting to see that for the random forest without NLP and the xgboost with NLP, the random forest has a lower mean absolute percent error but a higher RMSE than both xgboost models. The difference here is the absolute metric versus the root squared metric. This tells us that the random forest without NLP had a harder time with extreme values than the xgboost models. The linear regression models performed the most poorly. One other takeaway is that for both random forest and xgboost, the NLP addition improved the models, while it made the linear regression worse. All in all, though, being off by $3 billion may seem like a lot, but due to the wide span of values in our dataset, this model is pretty good. 


```{r}
exp_2_table
```

Here, for our second experiment, we can see that including NLP appears to have helped quite a bit. However, I believe that the splitting of the training and testing data may have led to somewhat of an inaccurate measurement here, as many factors in the financial stage column appeared in one dataset but not the other. This may have led to a lack of false positives or false negatives that may have occurred if we could have an abundantly large dataset to test and train on. The similarity in outcomes for the RF and xgboost with NLP sort of points to a potential issue there with factor levels. However, this experiment is somewhat hard to evaluate due to the proportion makeup of the unique values of the financial stages. 


```{r}
exp_3_table
```

Finally, for experiment three, in trying to predict the number of investors, we found a tight performance amongst the models. Here, there is no distinct winner that won in all metrics. It would depend on your purpose of using this model and whether or not you care about accommodating extreme values very well, but your model of choice could be either random forest without NLP or xgboost using NLP. It is cool to see NLP existing in one best performing model but not the other. However, aside from the porrly performing linear regressions, there was a pretty consistent MAPE of about 65-70% and an RMSE of 8.5-9. This is pretty high quality and could be useful to predict financial things about a startup. One other thing that is interesting to note is that when doing grid search, the metric used was RMSE, so these models are generally going to be biased towards performing well in the RMSE category. 

All in all, the models performed really well in all experiments, although it is hard to accurately assess experiment 2. It was also rewarding to see that NLP seemed to help the models perform better in more cases than not. 


## Github Repo Link

[https://github.com/STAT301III/final-project-variance-acknowledgers.git](https://github.com/STAT301III/final-project-variance-acknowledgers.git){target="_blank"}



